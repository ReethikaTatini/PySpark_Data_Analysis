# **PySpark_Data_Analysis**

This project focuses on analyzing Amazon reviews using PySpark within a Jupyter Notebook. It performs various operations to extract insights from the dataset,
such as finding items with the least and most ratings, identifying items with the longest reviews, transforming date formats, store data into database and converting
the data into Parquet file format.

# **Using Jupyter Notebook**

**Requirements**

*   Python
*   Jupyter Notebook
*   Apache Spark
*   PySpark


# **Steps to Execute**

**1.Installations**

*   Install Python and Jupyter Notebook.
*   Download and set up Apache Sparkon your local machine.
*   Install PySpark and findspark to enable the use of Spark within Jupyter Notebook.
  
**2. Configuration**

*   Set up environment variables(e.g.,'**SPARK_HOME**', '**JAVA_HOME**')to locate the Apache Spark Installation.

3.**Execution**

*   Open Jupyter Notebook
*   Download the datasets using this link  http://jmcauley.ucsd.edu/data/amazon/links.html
*  Unzip the dataset folder and place it in specific path.
*   Write the PySpark code which was provided in the jupyter notebook file.
*   Customize file paths, column names and transformations based on your dataset.
*   Run each cell containing PySpark code step-by-step to execute the operations.

4.**Analysis Results**

*   Find items with the least and most ratings.
*   Identifies the item with the longest review.
*   Transforms date format.
*   Convert PySpark dataframe into Pandas Dataframe.
*   Store the dataframe into database using Postgres/SQL server.
*   Convert the whole file into Parquet file.


# **Using Colab Notebook**

**Requirements**

*   Colab Notebook
*   Upload datasets into colab notebooks folder in Drive

1. **Setup Environment**

*   Access Google Colab by signing in to your Google account.
*   Create a new notebook or upload an existing notebook containing the necessary code.

2. **Installations**

*   Install PySpark and findspark
*   In colab, we have everything installed by default.
*   we need to mount google drive for path

3. **Execution**

*   Open Colab Notebook
*   Download the datasets using this link http://jmcauley.ucsd.edu/data/amazon/links.html
*   Unzip the dataset folder and place it in specific path.
*   Write the PySpark code which was provided in the jupyter notebook file.
*   Customize file paths, column names and transformations based on your dataset.
*   Run each cell containing PySpark code step-by-step to execute the operations.


4. **Analysis Results**

*   Find items with the least and most ratings.
*   Identifies the item with the longest review.
*   Transforms date format.
*  Convert PySpark dataframe into Pandas Dataframe.
*  Store the dataframe into database using Postgres/SQL server we need colab pro without pro account we can use Sqlite library.
*  Convert the whole file into Parquet file.








































#PySpark_Data_Analysis
This project focuses on analyzing Amazon reviews using PySpark within a Jupyter Notebook. It performs various operations to extract insights from the dataset,
such as finding items with the least and most ratings, identifying items with the longest reviews, transforming date formats, store data into database and converting
the data into Parquet file format.

# **Using Jupyter Notebook**

**Requirements**


*   Python
*   Jupyter Notebook

*   Apache Spark
*   PySpark


# **Steps to Execute**

**1.Installations**


*   Install Python and Jupyter Notebook.
*   Download and set up Apache Sparkon your local machine.

*   Install PySpark and findspark to enable the use of Spark within Jupyter Notebook.

**2. Configuration**

*   Set up environment variables(e.g.,'**SPARK_HOME**', '**JAVA_HOME**')to locate the Apache Spark Installation.

3.**Execution**

*   Open Jupyter Notebook

*   Download the datasets using this link  http://jmcauley.ucsd.edu/data/amazon/links.html
*  Unzip the dataset folder and place it in specific path.


*   Write the PySpark code which was provided in the jupyter notebook file.

*   Customize file paths, column names and transformations based on your dataset.
*   Run each cell containing PySpark code step-by-step to execute the operations.

4.**Analysis Results**

*   Find items with the least and most ratings.

*   Identifies the item with the longest review.
*   Transforms date format.


*   Convert PySpark dataframe into Pandas Dataframe.

*   Store the dataframe into database using Postgres/SQL server.
*   Convert the whole file into Parquet file.


# **Using Colab Notebook**

**Requirements**


*   Colab Notebook
*   Upload datasets into colab notebooks folder in Drive


1. **Setup Environment**

*   Access Google Colab by signing in to your Google account.
*   Create a new notebook or upload an existing notebook containing the necessary code.

2. **Installations**

*   Install PySpark and findspark
*   In colab, we have everything installed by default.


*   we need to mount google drive for path

3. **Execution**

*   Open Colab Notebook
*   Download the datasets using this link http://jmcauley.ucsd.edu/data/amazon/links.html


*   Unzip the dataset folder and place it in specific path.
*   Write the PySpark code which was provided in the jupyter notebook file.

*   Customize file paths, column names and transformations based on your dataset.
*   Run each cell containing PySpark code step-by-step to execute the operations.


4. **Analysis Results**

*   Find items with the least and most ratings.
*   Identifies the item with the longest review.

*   Transforms date format.
*   Convert PySpark dataframe into Pandas Dataframe.

*   Store the dataframe into database using Postgres/SQL server we need colab pro.
*   Convert the whole file into Parquet file.




